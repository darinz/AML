# Probabilistic & Statistical Methods

This module covers statistical learning approaches and unsupervised learning techniques that model uncertainty and hidden patterns in data.

## Learning Objectives

By the end of this module, you will be able to:
- Implement Expectation-Maximization (EM) algorithm for latent variable models
- Build density estimation models using various statistical techniques
- Detect and handle outliers using robust estimation methods
- Understand probabilistic modeling and uncertainty quantification
- Apply these methods to unsupervised learning problems

## Topics Covered

### 1. EM and Latent Variables
- **Expectation-Maximization**: Iterative optimization for incomplete data
- **Gaussian Mixture Models (GMM)**: Clustering with probabilistic interpretation
- **Hidden Markov Models (HMM)**: Sequential data modeling
- **Latent Dirichlet Allocation (LDA)**: Topic modeling for documents
- **Missing Data Imputation**: Handling incomplete datasets

### 2. Density Estimation
- **Mixture of Gaussians (MoG)**: Probabilistic clustering and density modeling
- **Histograms**: Non-parametric density estimation
- **Kernel Density Estimation (KDE)**: Smooth density estimation
- **Parametric vs Non-parametric**: Choosing appropriate density models
- **Multivariate Density Estimation**: High-dimensional density modeling

### 3. Outliers and Robust Estimation
- **Anomaly Detection**: Identifying unusual data points
- **Robust Statistics**: Median, MAD, robust covariance
- **Isolation Forest**: Tree-based anomaly detection
- **One-Class SVM**: Learning normal data boundaries
- **Statistical Tests**: Z-score, IQR, Mahalanobis distance

## Practical Applications

- **Fraud Detection**: Identifying anomalous transactions
- **Quality Control**: Detecting defective products in manufacturing
- **Network Security**: Intrusion detection systems
- **Medical Imaging**: Tumor detection and segmentation
- **Text Analysis**: Topic modeling and document clustering

## Implementation Focus

This module emphasizes **statistical rigor and robustness**:
- Implement EM algorithm from scratch for GMM
- Build KDE with different kernel functions
- Code robust estimation methods
- Create anomaly detection systems
- Handle edge cases and numerical stability

## Key Concepts

- **Likelihood and Maximum Likelihood**: Understanding probabilistic inference
- **Bayesian vs Frequentist**: Different approaches to statistical modeling
- **Uncertainty Quantification**: Confidence intervals and credible regions
- **Model Selection**: AIC, BIC, and cross-validation for probabilistic models

## Mathematical Prerequisites

- Probability theory (distributions, Bayes' rule)
- Linear algebra (eigenvalues, matrix operations)
- Calculus (derivatives, optimization)
- Basic statistics (mean, variance, correlation)

## Prerequisites

- Completion of Linear Models & Classical ML module
- Comfort with probability and statistics
- Understanding of optimization algorithms

## Next Steps

After completing this module, you'll be ready for **Neural Networks & Deep Learning Foundations** where you'll learn about modern deep learning approaches. 